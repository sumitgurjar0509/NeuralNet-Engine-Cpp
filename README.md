# ğŸ“˜ NeuralNet-Engine-Cpp

A from-scratch neural network engine in C++ demonstrating core deep learning principles without external ML frameworks.

ğŸ“Œ Overview

NeuralNet-Engine-Cpp is a lightweight, educational neural network engine implemented entirely from scratch in C++.
The project focuses on clarity, correctness, and understanding, rather than using high-level libraries.

The engine implements:

Fully connected (dense) neural networks

Forward propagation

Backpropagation with gradient descent

Training and evaluation on the MNIST handwritten digit dataset

This project is designed to demonstrate how neural networks actually work internally, making it ideal for:

Learning ML fundamentals

Understanding backpropagation at code level

Showcasing strong C++ + ML foundations on a resume

## ğŸ§  Key Features

âœ… Pure C++ implementation (no TensorFlow / PyTorch)

âœ… Modular neural network components

âœ… Forward & backward propagation

âœ… Gradient descent optimization

âœ… MNIST dataset support

âœ… CMake build system

âœ… Clean, readable, and extensible codebase

## ğŸ—ï¸ Project Architecture

NeuralNet-Engine-Cpp/
â”‚
â”œâ”€â”€ CMakeLists.txt        # Build configuration
â”œâ”€â”€ README.md             # Project documentation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.cpp          # Entry point and training loop
â”‚   â”œâ”€â”€ NeuralNetwork.cpp # Neural network logic
â”‚   â”œâ”€â”€ NeuralNetwork.h
â”‚   â”œâ”€â”€ Layer.cpp         # Layer abstraction
â”‚   â”œâ”€â”€ Layer.h
â”‚   â”œâ”€â”€ Utils.cpp         # Helper utilities
â”‚   â””â”€â”€ Utils.h
â”‚
â””â”€â”€ data/
    â””â”€â”€ mnist/            # MNIST dataset files

Design philosophy:
Each component (network, layers, utilities) is kept separate to ensure maintainability and extensibility.

## âš™ï¸ How It Works (High-Level)

Input Layer

Reads pixel values from MNIST images

Normalizes input data

Hidden Layers

Fully connected layers

Activation functions applied

Output Layer

Produces probability scores for digits (0â€“9)

Training

Loss computed using prediction vs ground truth

Gradients calculated using backpropagation

Weights updated using gradient descent

## ğŸ” Training Pipeline
Input â†’ Forward Pass â†’ Loss Calculation
      â†’ Backpropagation â†’ Weight Update
      â†’ Repeat for Epochs

This explicit pipeline makes the learning process transparent and debuggable, unlike black-box frameworks.

## ğŸ“Š Dataset

MNIST Handwritten Digits

60,000 training images

10,000 test images

Each image: 28Ã—28 grayscale

The dataset is widely used as a benchmark for classification models.

## ğŸ§ª How to Build & Run
ğŸ”§ Prerequisites

C++17 compatible compiler

CMake (â‰¥ 3.10)

ğŸ—ï¸ Build

```mkdir build
cd build
cmake ..
make ```

â–¶ï¸ Run
```./NeuralNetEngine
```
## ğŸ“ˆ Results

The network successfully learns digit classification

Accuracy improves steadily across epochs

Demonstrates correct gradient flow and convergence behavior

Note: This engine prioritizes correctness and clarity over performance.

## ğŸ¯ Learning Outcomes

By building this project, I gained:

Deep understanding of backpropagation

Hands-on experience with numerical stability

Stronger grasp of ML mathematics

Confidence in implementing complex systems in C++

## ğŸš€ Possible Extensions

Add configurable activation functions (ReLU, Tanh)

Support for different optimizers (Adam, RMSProp)

Modular loss functions

Batch normalization

GPU acceleration (future work)

Unit tests for layers and gradients